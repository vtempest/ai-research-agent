---
title: Generate language model reply using agent prompts
description: ''
full: true
_openapi:
  method: POST
  route: /agents
  toc: []
  structuredData:
    headings: []
    contents:
      - content: >
          ![Agents](https://i.imgur.com/ttOXv0p.jpeg)

          - ‚ùì **Inputs**: üëÑ Language Intelligence Provider, üîë API Key,  ü§ñ
          agent template name, üß† model name and options,
            and üÜé context variables for that agent
          - ü§ñ **Agent Instruction Templates**:
          [LangHub](https://smith.langchain.com/hub) template or custom: 
            question(query, chat_history), summarize-bullets(article), summarize(article), 
            suggest-followups(chat_history, article), answer-cite-sources(context, chat_history, query),
            query-resolution(chat_history, query), knowledge-graph-nodes(query, article), 
            summary-longtext(summaries)
          - üß† **How Language Models Work**: Language models learn from billions
          of text examples to 

          identify statistical patterns and structures across diverse sources,
          converting words into
           high-dimensional vectors‚Äînumerical lists that capture meaning and relationships between concepts. 
           These mathematical representations allow models to understand that "king/queen" share properties 
           and "Paris/France" mirrors "Tokyo/Japan" through their transformer architecture, a neural network 
           backbone that processes information through multiple layers of analysis. The attention mechanism 
           enables the system to dynamically focus on relevant parts of input text when generating each word, 
           maintaining context like humans tracking conversation threads, while calculating probability scores 
           across the entire vocabulary for each word position based on processed context. Rather than retrieving
            stored responses, models create novel text by selecting the most probable words given learned 
            patterns, maintaining coherence across long passages while adapting to specific prompt nuances
             through deep pattern recognition.  
             **Self-Attention**: Each word creates three representations: Query (what it's looking for), Key (what
              it offers), and Value (its actual content). For example, in "The cat sat on the mat," the word "cat" 
              has a Query vector that searches for actions, a Key vector that advertises itself as a subject, and 
              a Value vector containing its semantic meaning as an animal. The attention mechanism calculates how
              much "cat" should focus on other words by comparing its Query with their Keys - finding high 
              similarity with "sat" (the action) - then combines the corresponding Value vectors to create a
               contextualized representation where "cat" now understands it's the one doing the sitting.
             
          - üìö **Learning Resources**:
             [LLM Training Example](https://github.com/vtempest/ai-research-agent/blob/master/packages/neural-net/src/train/predict-next-word.js),
             [LangChain ReactAgent Tools](https://medium.com/@terrycho/how-langchain-agent-works-internally-trace-by-using-langsmith-df23766e7fb4),
             [Hugging Face Tutorials](https://huggingface.co/learn), [OpenAI Cookbook](https://cookbook.openai.com),
             [Transformer Overview](https://jalammar.github.io/illustrated-transformer/),
             [Building Transformer Guide](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch),
             [PyTorch Overview](https://www.learnpytorch.io/pytorch_cheatsheet/)
             
          ### üëÑ  Language Intelligence Providers (LIPs)

            | üëÑ Provider | ü§ñ Model Families | üìö Docs | üîë Keys | üí∞ Valuation | üí∏ Revenue (2024) | üí≤ Cost (1M Output) |
            | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
            | **XAI** | Grok, Grok Vision | [Docs](https://docs.x.ai/docs#models) | [Keys](https://console.x.ai/) | \$80B | \$100M | \$15.00 |
            | **Groq** | Llama, DeepSeek, Gemini, Mistral | [Docs](https://console.groq.com/docs/overview) | [Keys](https://console.groq.com/keys) | \$2.8B | - | \$0.79 |
            | **Ollama** | llama, mistral, mixtral, vicuna, gemma, qwen, deepseek, openchat, openhermes, codelama, codegemma, llava, minicpm, wizardcoder, wizardmath, meditrion, falcon | [Docs](https://ollama.com/docs) | [Keys](https://ollama.com/settings/keys) | - | \$3.2M | \$0 |
            | **OpenAI** | o1, o1-mini, o4, o4-mini, gpt-4, gpt-4-turbo, gpt-4-omni | [Docs](https://platform.openai.com/docs/overview) | [Keys](https://platform.openai.com/api-keys) | \$300B | \$3.7B | \$8.00 |
            | **Anthropic** | Claude Sonnet, Claude Opus, Claude Haiku | [Docs](https://docs.anthropic.com/en/docs/welcome) | [Keys](https://console.anthropic.com/settings/keys) | \$61.5B | \$1B | \$15.00 |
            | **TogetherAI** | Llama, Mistral, Mixtral, Qwen, Gemma, WizardLM, DBRX, DeepSeek, Hermes, SOLAR, StripedHyena | [Docs](https://docs.together.ai/docs/quickstart) | [Keys](https://api.together.xyz/settings/api-keys) | \$3.3B | \$50M | \$0.90 |
            | **Perplexity** | Sonar, Sonar Deep Research | [Docs](https://docs.perplexity.ai/models/model-cards) | [Keys](https://www.perplexity.ai/account/api/keys) | \$18B | \$20M | \$15.00 |
            | **Cloudflare** | Llama, Gemma, Mistral, Phi, Qwen, DeepSeek, Hermes, SQL Coder, Code Llama | [Docs](https://developers.cloudflare.com/workers-ai/) | [Keys](https://dash.cloudflare.com/profile/api-tokens) | \$62.3B | \$1.67B | \$2.25 |
            | **Google** | Gemini | [Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) | [Keys](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview#api-keys) | - | ~$400M | \$10.00 |

          ![agent_arch_viz](https://i.imgur.com/bailW5n.gif)

          ![agent_arch_viz2](https://i.imgur.com/uW6E9VJ.gif)
---

![Agents](https://i.imgur.com/ttOXv0p.jpeg)
- ‚ùì **Inputs**: üëÑ Language Intelligence Provider, üîë API Key,  ü§ñ agent template name, üß† model name and options,
  and üÜé context variables for that agent
- ü§ñ **Agent Instruction Templates**: [LangHub](https://smith.langchain.com/hub) template or custom: 
  question(query, chat_history), summarize-bullets(article), summarize(article), 
  suggest-followups(chat_history, article), answer-cite-sources(context, chat_history, query),
  query-resolution(chat_history, query), knowledge-graph-nodes(query, article), 
  summary-longtext(summaries)
- üß† **How Language Models Work**: Language models learn from billions of text examples to 
identify statistical patterns and structures across diverse sources, converting words into
 high-dimensional vectors‚Äînumerical lists that capture meaning and relationships between concepts. 
 These mathematical representations allow models to understand that "king/queen" share properties 
 and "Paris/France" mirrors "Tokyo/Japan" through their transformer architecture, a neural network 
 backbone that processes information through multiple layers of analysis. The attention mechanism 
 enables the system to dynamically focus on relevant parts of input text when generating each word, 
 maintaining context like humans tracking conversation threads, while calculating probability scores 
 across the entire vocabulary for each word position based on processed context. Rather than retrieving
  stored responses, models create novel text by selecting the most probable words given learned 
  patterns, maintaining coherence across long passages while adapting to specific prompt nuances
   through deep pattern recognition.  
   **Self-Attention**: Each word creates three representations: Query (what it's looking for), Key (what
    it offers), and Value (its actual content). For example, in "The cat sat on the mat," the word "cat" 
    has a Query vector that searches for actions, a Key vector that advertises itself as a subject, and 
    a Value vector containing its semantic meaning as an animal. The attention mechanism calculates how
    much "cat" should focus on other words by comparing its Query with their Keys - finding high 
    similarity with "sat" (the action) - then combines the corresponding Value vectors to create a
     contextualized representation where "cat" now understands it's the one doing the sitting.
   
- üìö **Learning Resources**:
   [LLM Training Example](https://github.com/vtempest/ai-research-agent/blob/master/packages/neural-net/src/train/predict-next-word.js),
   [LangChain ReactAgent Tools](https://medium.com/@terrycho/how-langchain-agent-works-internally-trace-by-using-langsmith-df23766e7fb4),
   [Hugging Face Tutorials](https://huggingface.co/learn), [OpenAI Cookbook](https://cookbook.openai.com),
   [Transformer Overview](https://jalammar.github.io/illustrated-transformer/),
   [Building Transformer Guide](https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch),
   [PyTorch Overview](https://www.learnpytorch.io/pytorch_cheatsheet/)
   
### üëÑ  Language Intelligence Providers (LIPs)

  | üëÑ Provider | ü§ñ Model Families | üìö Docs | üîë Keys | üí∞ Valuation | üí∏ Revenue (2024) | üí≤ Cost (1M Output) |
  | :-- | :-- | :-- | :-- | :-- | :-- | :-- |
  | **XAI** | Grok, Grok Vision | [Docs](https://docs.x.ai/docs#models) | [Keys](https://console.x.ai/) | \$80B | \$100M | \$15.00 |
  | **Groq** | Llama, DeepSeek, Gemini, Mistral | [Docs](https://console.groq.com/docs/overview) | [Keys](https://console.groq.com/keys) | \$2.8B | - | \$0.79 |
  | **Ollama** | llama, mistral, mixtral, vicuna, gemma, qwen, deepseek, openchat, openhermes, codelama, codegemma, llava, minicpm, wizardcoder, wizardmath, meditrion, falcon | [Docs](https://ollama.com/docs) | [Keys](https://ollama.com/settings/keys) | - | \$3.2M | \$0 |
  | **OpenAI** | o1, o1-mini, o4, o4-mini, gpt-4, gpt-4-turbo, gpt-4-omni | [Docs](https://platform.openai.com/docs/overview) | [Keys](https://platform.openai.com/api-keys) | \$300B | \$3.7B | \$8.00 |
  | **Anthropic** | Claude Sonnet, Claude Opus, Claude Haiku | [Docs](https://docs.anthropic.com/en/docs/welcome) | [Keys](https://console.anthropic.com/settings/keys) | \$61.5B | \$1B | \$15.00 |
  | **TogetherAI** | Llama, Mistral, Mixtral, Qwen, Gemma, WizardLM, DBRX, DeepSeek, Hermes, SOLAR, StripedHyena | [Docs](https://docs.together.ai/docs/quickstart) | [Keys](https://api.together.xyz/settings/api-keys) | \$3.3B | \$50M | \$0.90 |
  | **Perplexity** | Sonar, Sonar Deep Research | [Docs](https://docs.perplexity.ai/models/model-cards) | [Keys](https://www.perplexity.ai/account/api/keys) | \$18B | \$20M | \$15.00 |
  | **Cloudflare** | Llama, Gemma, Mistral, Phi, Qwen, DeepSeek, Hermes, SQL Coder, Code Llama | [Docs](https://developers.cloudflare.com/workers-ai/) | [Keys](https://dash.cloudflare.com/profile/api-tokens) | \$62.3B | \$1.67B | \$2.25 |
  | **Google** | Gemini | [Docs](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) | [Keys](https://cloud.google.com/vertex-ai/generative-ai/docs/start/express-mode/overview#api-keys) | - | ~$400M | \$10.00 |

![agent_arch_viz](https://i.imgur.com/bailW5n.gif)
![agent_arch_viz2](https://i.imgur.com/uW6E9VJ.gif)


<APIPage document={"./openapi.yml"} operations={[{"path":"/agents","method":"post"}]} webhooks={[]} hasHead={false} />