"use strict";(self.webpackChunkqwksearch_api_docs=self.webpackChunkqwksearch_api_docs||[]).push([[7847],{6009:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"neural-net/next-word-prediction/scripts/predict-next-word","title":"predict-next-word","description":"neural-net-training / next-word-prediction/scripts/predict-next-word","source":"@site/src/neural-net/next-word-prediction/scripts/predict-next-word.md","sourceDirName":"neural-net/next-word-prediction/scripts","slug":"/neural-net/next-word-prediction/scripts/predict-next-word","permalink":"/docs/neural-net/next-word-prediction/scripts/predict-next-word","draft":false,"unlisted":false,"editUrl":"https://github.com/vtempest/ai-research-agent/tree/master/apps/docs/src/neural-net/next-word-prediction/scripts/predict-next-word.md","tags":[],"version":"current","frontMatter":{},"sidebar":"default","previous":{"title":"neural-net-tf","permalink":"/docs/neural-net/neural-net-tensors/neural-net-tf"},"next":{"title":"predict-statistics","permalink":"/docs/neural-net/statistics/predict-statistics"}}');var i=r(31085),s=r(71184);const o={},a=void 0,d={},l=[{value:"Transformer",id:"transformer",level:2},{value:"trainNextWordPrediction()",id:"trainnextwordprediction",level:2},{value:"Predict Next Word Based On Vectors of Learned Context Patterns in Training Examples",id:"predict-next-word-based-on-vectors-of-learned-context-patterns-in-training-examples",level:3},{value:"Returns",id:"returns",level:3},{value:"Example",id:"example",level:3},{value:"Author",id:"author",level:3},{value:"See",id:"see",level:3}];function c(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"/docs/neural-net/modules",children:"neural-net-training"})," / next-word-prediction/scripts/predict-next-word"]}),"\n",(0,i.jsx)(n.h2,{id:"transformer",children:"Transformer"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-ts",children:"Transformer: any;\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"trainnextwordprediction",children:"trainNextWordPrediction()"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-ts",children:"function trainNextWordPrediction(): Promise<Object>;\n"})}),"\n",(0,i.jsxs)(n.p,{children:["Defined in: ",(0,i.jsx)(n.a,{href:"https://github.com/vtempest/ai-research-agent/tree/master/packages/neural-net/src/next-word-prediction/scripts/predict-next-word.js#L88",children:"next-word-prediction/scripts/predict-next-word.js:88"})]}),"\n",(0,i.jsx)(n.p,{children:"======================================================================================="}),"\n",(0,i.jsx)(n.h3,{id:"predict-next-word-based-on-vectors-of-learned-context-patterns-in-training-examples",children:"Predict Next Word Based On Vectors of Learned Context Patterns in Training Examples"}),"\n",(0,i.jsx)(n.p,{children:"======================================================================================="}),"\n",(0,i.jsx)(n.p,{children:"Comprehensive training function for a self-attention transformer using custom torch.js.\nThis function implements a decoder-only transformer architecture similar to GPT models,\ntraining it with synthetic data using the Adam optimizer and GPU.js for acceleration.\nIn real applications, this would be:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Tokenized text data (e.g., using BPE, WordPiece, or SentencePiece)"}),"\n",(0,i.jsx)(n.li,{children:"Loaded from datasets like WikiText, BookCorpus, or Common Crawl"}),"\n",(0,i.jsx)(n.li,{children:"Preprocessed with appropriate padding and attention masks"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Key architectural components:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Token and positional embeddings"}),"\n",(0,i.jsx)(n.li,{children:"Multi-head self-attention blocks"}),"\n",(0,i.jsx)(n.li,{children:"Layer normalization"}),"\n",(0,i.jsx)(n.li,{children:"Linear output projection"}),"\n",(0,i.jsx)(n.li,{children:"Cross-entropy loss computation"}),"\n",(0,i.jsx)(n.li,{children:"Adam optimization with backpropagation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Advanced usage scenarios:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Fine-tuning on domain-specific data"}),"\n",(0,i.jsx)(n.li,{children:"Transfer learning from pre-trained weights"}),"\n",(0,i.jsx)(n.li,{children:"Ensemble methods with multiple trained models"}),"\n",(0,i.jsx)(n.li,{children:"Model compression and quantization"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"returns",children:"Returns"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"Promise"}),"<",(0,i.jsx)(n.code,{children:"Object"}),">"]}),"\n",(0,i.jsx)(n.p,{children:"Training results containing final loss and model"}),"\n",(0,i.jsx)(n.h3,{id:"example",children:"Example"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-javascript",children:"import { trainNextWordPrediction, Transformer } from './transformer-training.js';\n\n// Train a new model\nconst results = await trainNextWordPrediction();\nconsole.log('Training completed with final loss:', results.finalLoss);\n\n// Generate a language response based using the trained model\nconst model = results.model;\nconst predictions = model.forward(inputTokens);\n"})}),"\n",(0,i.jsx)(n.h3,{id:"author",children:"Author"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.a,{href:"https://github.com/vtempest",children:"vtempest"})}),"\n",(0,i.jsx)(n.h3,{id:"see",children:"See"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Original Transformer paper: ",(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/1706.03762",children:"https://arxiv.org/abs/1706.03762"}),' ("Attention Is All You Need")']}),"\n",(0,i.jsxs)(n.li,{children:["PyTorch Transformer tutorial: ",(0,i.jsx)(n.a,{href:"https://pytorch.org/tutorials/beginner/transformer_tutorial.html",children:"https://pytorch.org/tutorials/beginner/transformer_tutorial.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["Annotated Transformer: ",(0,i.jsx)(n.a,{href:"https://nlp.seas.harvard.edu/2018/04/03/attention.html",children:"https://nlp.seas.harvard.edu/2018/04/03/attention.html"})]}),"\n",(0,i.jsxs)(n.li,{children:["GPU.js documentation: ",(0,i.jsx)(n.a,{href:"https://gpu.rocks/",children:"https://gpu.rocks/"})]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},71184:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var t=r(14041);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);