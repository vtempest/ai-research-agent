"""
ğŸ§  Transformer Neural Network Training with Tinygrad

This implements a decoder-only transformer architecture that learns statistical patterns 
from text to predict the next word in a sequence. Language models work by converting words 
into high-dimensional vectorsâ€”numerical lists that capture meaning and relationships between 
concepts. These mathematical representations allow models to understand that "king/queen" 
share properties and "Paris/France" mirrors "Tokyo/Japan" through their transformer 
architecture, a neural network backbone that processes information through multiple layers.

The attention mechanism enables the system to dynamically focus on relevant parts of input 
text when generating each word, maintaining context like humans tracking conversation threads, 
while calculating probability scores across the entire vocabulary for each word position 
based on processed context. Rather than retrieving stored responses, models create novel 
text by selecting the most probable words given learned patterns.

ğŸ” Self-Attention Mechanism:
Each word creates three representations: Query (what it's looking for), Key (what it offers), 
and Value (its actual content). For example, in "The cat sat on the mat," the word "cat" 
has a Query vector that searches for actions, a Key vector that advertises itself as a 
subject, and a Value vector containing its semantic meaning as an animal. The attention 
mechanism calculates how much "cat" should focus on other words by comparing its Query 
with their Keys - finding high similarity with "sat" (the action) - then combines the 
corresponding Value vectors to create a contextualized representation where "cat" now 
understands it's the one doing the sitting.

ğŸ“š Learning Resources:
- LLM Training Example: https://github.com/vtempest/ai-research-agent/blob/master/packages/neural-net/src/train/predict-next-word.js
- Transformer Overview: https://jalammar.github.io/illustrated-transformer/
- Building Transformer Guide: https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch
- Hugging Face Tutorials: https://huggingface.co/learn
- OpenAI Cookbook: https://cookbook.openai.com
- PyTorch Overview: https://www.learnpytorch.io/pytorch_cheatsheet/
- Original Transformer Paper: https://arxiv.org/abs/1706.03762 ("Attention Is All You Need")
- GPT Paper: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
- Tinygrad Documentation: https://docs.tinygrad.org/
"""

from tinygrad.tensor import Tensor
from tinygrad.nn import Linear, LayerNorm, Embedding
from tinygrad.nn.optim import Adam
import math
import numpy as np


class LearnedPositionalEmbeddings:
    """
    ğŸ¯ Positional Information Encoder
    
    Since attention mechanisms are permutation-invariant (they don't naturally understand 
    word order), we need to inject positional information. This layer learns position 
    embeddings that get added to token embeddings, teaching the model that word order matters.
    
    Think of it like adding GPS coordinates to each word so the model knows "The cat sat" 
    vs "Sat the cat" are different meanings.
    
    Args:
        sequence_max_length: Maximum number of words in a sequence
        embedding_dimension: Size of the vector representation for each position
    """
    def __init__(self, sequence_max_length: int, embedding_dimension: int):
        # Initialize random position vectors that will be learned during training
        # Each position gets its own unique vector representation
        self.position_vectors = Tensor.randn(sequence_max_length, embedding_dimension) * 0.02
        
    def __call__(self, token_indices):
        # token_indices shape: [batch_size, sequence_length, 1] for token IDs
        # Extract how many positions we need for this sequence
        sequence_length = token_indices.shape[1]
        position_embeddings = self.position_vectors[:sequence_length, :]  # [seq_len, embedding_dim]
        
        # Broadcast to match batch size: [batch_size, sequence_length, embedding_dimension]
        return position_embeddings.unsqueeze(0).expand(
            token_indices.shape[0], sequence_length, self.position_vectors.shape[1]
        )


class MultiHeadSelfAttention:
    """
    ğŸ¯ Multi-Head Self-Attention: The Core Intelligence Mechanism
    
    This is where the magic happens! Each word looks at every other word in the sentence 
    and decides how much attention to pay to each one. Multiple "heads" allow the model 
    to attend to different types of relationships simultaneously.
    
    For example, one head might focus on subject-verb relationships while another focuses 
    on noun-adjective connections. Each head creates Query, Key, and Value representations:
    
    - Query: "What am I looking for?" (e.g., "cat" looking for actions)
    - Key: "What do I offer?" (e.g., "sat" advertising itself as an action) 
    - Value: "What is my actual meaning?" (the semantic content)
    
    The attention score is calculated as: Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
    
    Reference: https://jalammar.github.io/illustrated-transformer/
    
    Args:
        embedding_dimension: Hidden size of embeddings
        num_attention_heads: Number of parallel attention mechanisms
        sequence_max_length: Maximum sequence length for causal masking
        dropout_probability: Regularization (not used in this implementation)
    """
    def __init__(self, embedding_dimension: int, num_attention_heads: int, 
                 sequence_max_length: int, dropout_probability: float = 0.0):
        assert embedding_dimension % num_attention_heads == 0, \
            "embedding_dimension must be divisible by num_attention_heads"
        
        self.embedding_dimension = embedding_dimension
        self.num_attention_heads = num_attention_heads
        self.head_dimension = embedding_dimension // num_attention_heads  # Size per attention head
        self.sequence_max_length = sequence_max_length
        
        # Linear transformations to create Query, Key, and Value representations
        # Each gets its own learned transformation matrix
        self.query_projection = Linear(embedding_dimension, embedding_dimension)
        self.key_projection = Linear(embedding_dimension, embedding_dimension)
        self.value_projection = Linear(embedding_dimension, embedding_dimension)
        self.output_projection = Linear(embedding_dimension, embedding_dimension)
        
        # ğŸš« Causal Mask: Prevents cheating by looking at future words
        # In language modeling, we can only use past and present context to predict the next word
        # This creates a lower triangular matrix where 1 = "can attend", 0 = "cannot attend"
        causal_mask = Tensor.ones(sequence_max_length, sequence_max_length).tril(diagonal=0)
        self.register_buffer('causal_attention_mask', causal_mask)
        
    def register_buffer(self, buffer_name: str, tensor_data: Tensor):
        """Store the causal mask as a model component"""
        setattr(self, buffer_name, tensor_data)
        
    def __call__(self, hidden_states):
        batch_size, sequence_length, embedding_dimension = hidden_states.shape
        
        # ğŸ”„ Generate Query, Key, Value representations for each word
        queries = self.query_projection(hidden_states)  # [batch, seq_len, embed_dim]
        keys = self.key_projection(hidden_states)       # [batch, seq_len, embed_dim]
        values = self.value_projection(hidden_states)   # [batch, seq_len, embed_dim]
        
        # ğŸ§© Reshape for multi-head attention: split embedding_dimension across heads
        # [batch, seq_len, embed_dim] -> [batch, seq_len, num_heads, head_dim] -> [batch, num_heads, seq_len, head_dim]
        queries = queries.reshape(batch_size, sequence_length, self.num_attention_heads, self.head_dimension).transpose(1, 2)
        keys = keys.reshape(batch_size, sequence_length, self.num_attention_heads, self.head_dimension).transpose(1, 2)
        values = values.reshape(batch_size, sequence_length, self.num_attention_heads, self.head_dimension).transpose(1, 2)
        
        # ğŸ¯ Scaled Dot-Product Attention
        # Calculate attention scores: how much should each word attend to every other word?
        # QK^T gives us similarity scores, scaled by âˆšd_k for numerical stability
        attention_scores = queries.dot(keys.transpose(-2, -1)) / math.sqrt(self.head_dimension)
        
        # ğŸš« Apply causal masking to prevent attending to future positions
        current_mask = self.causal_attention_mask[:sequence_length, :sequence_length]
        # Set masked positions to large negative value so softmax makes them â‰ˆ 0
        attention_scores = attention_scores.masked_fill(current_mask == 0, -1e9)
        
        # ğŸ“Š Convert scores to probabilities: each word's attention sums to 1
        attention_weights = attention_scores.softmax(axis=-1)
        
        # ğŸ­ Apply attention weights to values: weighted combination of meanings
        attention_output = attention_weights.dot(values)
        
        # ğŸ”„ Reshape back to original format: [batch, num_heads, seq_len, head_dim] -> [batch, seq_len, embed_dim]
        attention_output = attention_output.transpose(1, 2).reshape(batch_size, sequence_length, embedding_dimension)
        
        # ğŸ¯ Final linear transformation to integrate multi-head outputs
        return self.output_projection(attention_output)


class PositionWiseFeedForward:
    """
    ğŸ§  Position-wise Feed-Forward Network: Deep Processing Layer
    
    After attention mechanisms identify what to focus on, this layer performs deep 
    processing on each word position independently. It's a two-layer neural network 
    with ReLU activation that transforms the attended representations.
    
    Think of it as giving each word time to "think deeply" about what it learned 
    from attending to other words. The standard expansion factor is 4x, so a 512-dim 
    input becomes 2048-dim in the middle layer before contracting back to 512-dim.
    
    Reference: https://arxiv.org/abs/1706.03762 Section 3.3
    
    Args:
        embedding_dimension: Input/output dimension size
        feed_forward_dimension: Internal expansion dimension (typically 4 * embedding_dimension)
    """
    def __init__(self, embedding_dimension: int, feed_forward_dimension: int = None):
        if feed_forward_dimension is None:
            feed_forward_dimension = 4 * embedding_dimension  # Standard 4x expansion
            
        self.expansion_layer = Linear(embedding_dimension, feed_forward_dimension)
        self.contraction_layer = Linear(feed_forward_dimension, embedding_dimension)
        
    def __call__(self, hidden_states):
        # ğŸ”„ Two-layer MLP: Expand -> ReLU -> Contract
        # ReLU adds non-linearity to enable complex pattern learning
        expanded_representation = self.expansion_layer(hidden_states).relu()
        processed_output = self.contraction_layer(expanded_representation)
        return processed_output


class TransformerDecoderBlock:
    """
    ğŸ—ï¸ Single Transformer Decoder Block: The Core Building Block
    
    This is the fundamental unit of transformer architecture, repeated multiple times 
    to build deep language understanding. Each block contains:
    
    1. ğŸ¯ Multi-head self-attention: Learn what to focus on
    2. ğŸ§  Feed-forward network: Process the attended information deeply
    3. ğŸ”„ Residual connections: Preserve original information flow
    4. ğŸ“Š Layer normalization: Stabilize training dynamics
    
    The architecture uses "pre-normalization" (LayerNorm before attention/FFN) which 
    provides better training stability compared to post-normalization.
    
    Reference: https://arxiv.org/abs/2002.04745 (Pre-LN Transformer)
    
    Args:
        embedding_dimension: Hidden dimension size throughout the block
        num_attention_heads: Number of parallel attention mechanisms
        sequence_max_length: Maximum sequence length for causal masking
        dropout_probability: Regularization probability (not implemented here)
    """
    def __init__(self, embedding_dimension: int, num_attention_heads: int, 
                 sequence_max_length: int, dropout_probability: float = 0.0):
        self.self_attention = MultiHeadSelfAttention(
            embedding_dimension, num_attention_heads, sequence_max_length, dropout_probability
        )
        self.feed_forward_network = PositionWiseFeedForward(embedding_dimension)
        
        # Layer normalization for stable training dynamics
        self.attention_layer_norm = LayerNorm(embedding_dimension)
        self.feed_forward_layer_norm = LayerNorm(embedding_dimension)
        
    def __call__(self, hidden_states):
        # ğŸ¯ Self-Attention Sub-layer with Pre-LN and Residual Connection
        # Pre-normalization: normalize first, then apply transformation
        normalized_states = self.attention_layer_norm(hidden_states)
        attention_output = self.self_attention(normalized_states)
        hidden_states = hidden_states + attention_output  # Residual connection preserves original info
        
        # ğŸ§  Feed-Forward Sub-layer with Pre-LN and Residual Connection
        normalized_states = self.feed_forward_layer_norm(hidden_states)
        feed_forward_output = self.feed_forward_network(normalized_states)
        hidden_states = hidden_states + feed_forward_output  # Another residual connection
        
        return hidden_states


class GPTStyleTransformer:
    """
    ğŸ¤– GPT-Style Transformer: Complete Language Model Architecture
    
    This implements a decoder-only transformer similar to GPT models that learn to predict 
    the next word in a sequence. Unlike encoder-decoder transformers (like the original), 
    this architecture only uses decoder blocks with causal masking for autoregressive 
    text generation.
    
    ğŸ—ï¸ Architecture Flow:
    1. ğŸ“ Token Embedding: Convert word IDs to dense vectors
    2. ğŸ“ Positional Embedding: Add position information
    3. ğŸ§  Multiple Transformer Blocks: Deep pattern recognition
    4. ğŸ“Š Final Layer Norm: Output stabilization  
    5. ğŸ¯ Language Modeling Head: Predict next word probabilities
    
    The model learns to compress human language patterns into billions of parameters,
    enabling it to generate coherent text by predicting one word at a time based on
    all previous context.
    
    References:
    - GPT Paper: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
    - Transformer Paper: https://arxiv.org/abs/1706.03762
    - Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/
    
    Args:
        vocabulary_size: Total number of unique tokens the model can handle
        embedding_dimension: Size of word vector representations
        sequence_max_length: Maximum number of words in input sequences
        num_attention_heads: Number of parallel attention mechanisms per block
        num_transformer_layers: Number of transformer blocks to stack
        dropout_probability: Regularization rate (disabled in this demo)
    """
    def __init__(self, vocabulary_size: int, embedding_dimension: int, sequence_max_length: int, 
                 num_attention_heads: int, num_transformer_layers: int = 2, dropout_probability: float = 0.0):
        
        # ğŸ“ Word Embedding: Maps discrete token IDs to continuous vector space
        # This learnable lookup table stores meaning representations for each vocabulary word
        self.token_embeddings = Embedding(vocabulary_size, embedding_dimension)
        
        # ğŸ“ Position Embedding: Adds sequence order information
        # Crucial since attention is permutation-invariant by default
        self.positional_embeddings = LearnedPositionalEmbeddings(sequence_max_length, embedding_dimension)
        
        # ğŸ§  Stack of Transformer Blocks: The core intelligence layers
        # Each block can learn increasingly complex language patterns
        self.transformer_blocks = [
            TransformerDecoderBlock(embedding_dimension, num_attention_heads, sequence_max_length, dropout_probability) 
            for _ in range(num_transformer_layers)
        ]
        
        # ğŸ“Š Final Layer Normalization: Stabilizes final representations
        self.final_layer_norm = LayerNorm(embedding_dimension)
        
        # ğŸ¯ Language Modeling Head: Projects hidden states to vocabulary probabilities
        # This linear layer predicts the probability of each word being next
        self.language_modeling_head = Linear(embedding_dimension, vocabulary_size)
        
    def __call__(self, input_token_ids):
        """
        ğŸ”„ Forward Pass: Convert Token IDs to Next-Word Predictions
        
        This function implements the complete forward computation:
        1. Convert tokens to embeddings and add positional info
        2. Process through multiple transformer blocks
        3. Apply final normalization and project to vocabulary
        
        Args:
            input_token_ids: Token IDs of shape [batch_size, sequence_length]
            
        Returns:
            Tensor: Logits of shape [batch_size, sequence_length, vocabulary_size]
                   Each position contains probability scores for all possible next words
        """
        # ğŸ“ Convert discrete tokens to continuous vector representations
        token_vectors = self.token_embeddings(input_token_ids)  # [batch, seq_len, embed_dim]
        
        # ğŸ“ Add positional information so model understands word order
        position_vectors = self.positional_embeddings(input_token_ids)  # [batch, seq_len, embed_dim]
        
        # ğŸ”„ Combine semantic (token) and positional information
        # This gives each word both meaning and position context
        hidden_representations = token_vectors + position_vectors
        
        # ğŸ§  Process through stack of transformer blocks
        # Each block adds another layer of language understanding
        for transformer_block in self.transformer_blocks:
            hidden_representations = transformer_block(hidden_representations)
            
        # ğŸ“Š Apply final normalization for stable outputs
        normalized_representations = self.final_layer_norm(hidden_representations)
        
        # ğŸ¯ Project to vocabulary size for next-word prediction
        # Each position gets probability scores for all possible next words
        next_word_logits = self.language_modeling_head(normalized_representations)
        
        return next_word_logits


def train_next_word_predictor():
    """
    ğŸš€ Comprehensive Transformer Training Pipeline
    
    This function demonstrates the complete process of training a language model to predict 
    the next word in sequences. In production, this would process billions of text examples 
    from books, websites, and articles to learn human language patterns.
    
    ğŸ¯ Training Process:
    1. ğŸ“Š Initialize model with random weights
    2. ğŸ“ Feed text sequences through the model  
    3. ğŸ“ˆ Calculate prediction errors using cross-entropy loss
    4. ğŸ”„ Adjust weights to minimize errors using backpropagation
    5. ğŸ” Repeat millions of times until the model learns language patterns
    
    The model learns by trying to predict the next word in billions of examples,
    gradually discovering grammar, facts, reasoning patterns, and world knowledge
    embedded in human text.
    
    Reference: https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch
    
    Returns:
        dict: Complete training results with model, losses, and metadata
    """
    
    # âš™ï¸ Model Architecture Configuration
    # These hyperparameters control the model's capacity and behavior
    vocabulary_size = 52        # Number of unique words/tokens (small for demo)
    embedding_dimension = 32    # Size of word vector representations  
    sequence_max_length = 16    # Maximum words per training example
    num_attention_heads = 4     # Parallel attention mechanisms (must divide embedding_dimension)
    num_transformer_layers = 2  # Depth of the neural network
    dropout_probability = 0.0   # Regularization (disabled for clearer demo)
    
    # ğŸ“ Training Configuration  
    batch_size = 8              # Number of text sequences processed simultaneously
    num_training_epochs = 30    # Number of complete passes through the data
    learning_rate = 5e-3        # How aggressively to update model weights
    
    print(f"ğŸ§  Initializing GPT-style transformer:")
    print(f"   ğŸ“š Vocabulary: {vocabulary_size} tokens")
    print(f"   ğŸ¯ Embedding size: {embedding_dimension} dimensions") 
    print(f"   ğŸ—ï¸ Architecture: {num_transformer_layers} layers, {num_attention_heads} attention heads")
    
    # ğŸ¤– Model Instantiation
    # Create the transformer with specified architecture
    language_model = GPTStyleTransformer(
        vocabulary_size=vocabulary_size,
        embedding_dimension=embedding_dimension,
        sequence_max_length=sequence_max_length,
        num_attention_heads=num_attention_heads,
        num_transformer_layers=num_transformer_layers,
        dropout_probability=dropout_probability
    )
    
    # ğŸ“ˆ Optimizer Setup: Adam for adaptive learning rate optimization
    # Adam adjusts learning rate per parameter based on gradient history
    # Reference: https://arxiv.org/abs/1412.6980
    all_model_parameters = [
        language_model.token_embeddings.weight,
        language_model.positional_embeddings.position_vectors,
        # Collect all transformer block parameters
        *[parameter for block in language_model.transformer_blocks 
          for layer in [block.self_attention.query_projection, block.self_attention.key_projection, 
                       block.self_attention.value_projection, block.self_attention.output_projection,
                       block.feed_forward_network.expansion_layer, block.feed_forward_network.contraction_layer,
                       block.attention_layer_norm, block.feed_forward_layer_norm] 
          for parameter in [layer.weight, getattr(layer, 'bias', None)] if parameter is not None],
        language_model.final_layer_norm.weight,
        language_model.final_layer_norm.bias,
        language_model.language_modeling_head.weight,
        language_model.language_modeling_head.bias
    ]
    
    optimizer = Adam(all_model_parameters, lr=learning_rate)
    
    # ğŸ“Š Synthetic Training Data Generation
    # In production, this would be real tokenized text from books, articles, etc.
    # Format: sequences of token IDs representing words
    input_sequences = Tensor(np.random.randint(0, vocabulary_size, (batch_size, sequence_max_length)))
    
    # ğŸ¯ Target Generation: Next word for each position
    # In real language modeling, targets are input sequences shifted by one position:
    # Input:  [START, "The", "cat", "sat"] 
    # Target: ["The", "cat", "sat", "END"]
    target_sequences = Tensor(np.random.randint(0, vocabulary_size, (batch_size, sequence_max_length)))
    
    print(f"ğŸ“ Generated training data:")
    print(f"   ğŸ“Š Input shape: {input_sequences.shape}")
    print(f"   ğŸ¯ Target shape: {target_sequences.shape}")
    print(f"   ğŸ“ˆ Total parameters to learn: {len([p for p in all_model_parameters if p is not None])}")
    
    # ğŸ“ Training Loop: The Learning Process
    print(f"\nğŸš€ Starting training for {num_training_epochs} epochs...")
    training_loss_history = []
    
    for epoch_number in range(num_training_epochs):
        # ğŸ”„ Forward Pass: Model makes predictions
        # Process input sequences through all transformer layers
        predicted_logits = language_model(input_sequences)  # [batch, seq_len, vocab_size]
        
        # ğŸ“Š Loss Calculation: How wrong are the predictions?
        # Cross-entropy measures the difference between predicted and actual next words
        # Reshape for loss calculation: flatten batch and sequence dimensions
        flattened_predictions = predicted_logits.reshape(-1, vocabulary_size)  # [batch*seq_len, vocab_size]
        flattened_targets = target_sequences.reshape(-1)                       # [batch*seq_len]
        
        # ğŸ“ˆ Cross-entropy loss: -log(probability of correct word)
        # Lower loss = better predictions
        prediction_loss = flattened_predictions.sparse_categorical_crossentropy(flattened_targets)
        
        # ğŸ”„ Backward Pass: Calculate how to improve
        optimizer.zero_grad()          # Clear previous gradients
        prediction_loss.backward()     # Compute gradients via backpropagation  
        optimizer.step()               # Update model weights
        
        # ğŸ“Š Progress Monitoring
        current_loss = prediction_loss.item()
        training_loss_history.append(current_loss)
        
        if epoch_number % 5 == 0:  # Report every 5 epochs
            print(f"   ğŸ“ˆ Epoch {epoch_number:3d} | Loss: {current_loss:.4f}")
    
    final_training_loss = training_loss_history[-1]
    print(f"\nâœ… Training completed! Final loss: {final_training_loss:.4f}")
    
    # ğŸ“Š Return Complete Training Results
    return {
        'trained_model': language_model,
        'final_loss': final_training_loss,
        'loss_history': training_loss_history,
        'training_successful': True,
        'total_epochs': num_training_epochs,
        'model_configuration': {
            'vocabulary_size': vocabulary_size,
            'embedding_dimension': embedding_dimension,
            'sequence_max_length': sequence_max_length,
            'num_attention_heads': num_attention_heads,
            'num_transformer_layers': num_transformer_layers,
            'batch_size': batch_size,
            'learning_rate': learning_rate
        }
    }


def generate_text_autoregressive(trained_model, starting_tokens, max_new_tokens=10, sampling_temperature=1.0):
    """
    ğŸ­ Autoregressive Text Generation: Creating New Text Word by Word
    
    This function demonstrates how language models generate text by predicting one word 
    at a time and feeding each prediction back as input for the next prediction. This 
    autoregressive process allows models to generate coherent text of arbitrary length.
    
    ğŸ”„ Generation Process:
    1. Start with initial tokens (prompt)
    2. Model predicts probability distribution over next words
    3. Sample a word from this distribution  
    4. Append sampled word to sequence
    5. Repeat steps 2-4 for desired length
    
    The sampling temperature controls randomness: lower values make the model more 
    deterministic (always pick most likely word), higher values add creativity.
    
    Reference: https://huggingface.co/blog/how-to-generate
    
    Args:
        trained_model: The trained transformer model
        starting_tokens: Initial sequence to continue [1, seq_len]
        max_new_tokens: Number of new words to generate
        sampling_temperature: Controls randomness (1.0 = normal, <1.0 = focused, >1.0 = creative)
    
    Returns:
        Tensor: Extended sequence with generated tokens
    """
    generated_sequence = starting_tokens.clone()
    
    print(f"ğŸ­ Generating {max_new_tokens} new tokens...")
    print(f"   ğŸŒ¡ï¸ Temperature: {sampling_temperature}")
    
    for step in range(max_new_tokens):
        # ğŸ§  Get model predictions for current sequence (no gradient computation needed)
        with Tensor.no_grad():  # Disable gradients for efficiency during inference
            next_word_logits = trained_model(generated_sequence)  # [1, seq_len, vocab_size]
            
            # ğŸ¯ Extract logits for the last position (what comes next?)
            final_position_logits = next_word_logits[0, -1, :] / sampling_temperature  # [vocab_size]
            
            # ğŸ“Š Convert logits to probabilities and sample next word
            word_probabilities = final_position_logits.softmax(axis=-1)
            next_token = word_probabilities.multinomial(1)  # Sample one token
            
            # ğŸ”„ Append generated token to sequence for next iteration
            generated_sequence = generated_sequence.cat(
                next_token.unsqueeze(0).unsqueeze(0), dim=1
            )
    
    return generated_sequence


def main():
    """ğŸš€ Main training and demonstration function"""
    # ğŸš€ Run the Complete Training and Generation Demo
    print("=" * 80)
    print("ğŸ§  TRANSFORMER LANGUAGE MODEL TRAINING WITH TINYGRAD")
    print("=" * 80)
    print("This demo shows how transformers learn to predict the next word in sequences,")
    print("the fundamental task that enables language understanding and generation.\n")
    
    # ğŸ“ Train the Language Model
    training_results = train_next_word_predictor()
    
    # ğŸ“Š Display Training Summary
    print("\n" + "=" * 80)
    print("ğŸ“Š TRAINING SUMMARY")
    print("=" * 80)
    print(f"âœ… Training successful: {training_results['training_successful']}")
    print(f"ğŸ“‰ Final loss: {training_results['final_loss']:.4f}")
    print(f"ğŸ“ˆ Total epochs: {training_results['total_epochs']}")
    print(f"ğŸ—ï¸ Model configuration: {training_results['model_configuration']}")
    
    # ğŸ­ Demonstrate Text Generation
    print("\n" + "=" * 80)
    print("ğŸ­ AUTOREGRESSIVE TEXT GENERATION DEMO")
    print("=" * 80)
    print("Showing how the model generates new text by predicting one word at a time...")
    
    trained_model = training_results['trained_model']
    vocab_size = training_results['model_configuration']['vocabulary_size']
    
    # ğŸ² Create random starting sequence (in practice, this would be real text tokens)
    initial_tokens = Tensor(np.random.randint(0, vocab_size, (1, 5)))
    print(f"ğŸ² Starting tokens: {initial_tokens.numpy().tolist()}")
    
    # ğŸ­ Generate continuation
    extended_sequence = generate_text_autoregressive(
        trained_model, initial_tokens, max_new_tokens=8, sampling_temperature=1.0
    )
    print(f"ğŸ¯ Generated sequence: {extended_sequence.numpy().tolist()}")
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ UNDERSTANDING WHAT HAPPENED")
    print("=" * 80)
    print("ğŸ§  The model learned statistical patterns from the training data and used")
    print("   those patterns to generate new sequences token by token.")
    print("ğŸ“ In real applications, you would:")
    print("   â€¢ Use billions of real text examples instead of random data")
    print("   â€¢ Train for thousands of epochs on massive datasets")
    print("   â€¢ Implement proper tokenization (BPE, WordPiece, etc.)")
    print("   â€¢ Use validation data to prevent overfitting") 
    print("   â€¢ Scale to billions of parameters for better performance")
    print("\nğŸ“š Learn more:")
    print("   â€¢ Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/")
    print("   â€¢ Hugging Face Course: https://huggingface.co/learn")
    print("   â€¢ Original Paper: https://arxiv.org/abs/1706.03762")
    print("   â€¢ LLM Training Example: https://github.com/vtempest/ai-research-agent/blob/master/packages/neural-net/src/train/predict-next-word.js")
    print("   â€¢ Building Transformers Guide: https://www.datacamp.com/tutorial/building-a-transformer-with-py-torch")
    print("   â€¢ OpenAI Cookbook: https://cookbook.openai.com")
    print("   â€¢ PyTorch Learning: https://www.learnpytorch.io/pytorch_cheatsheet/")


if __name__ == "__main__":
    # Run the main training demo first
    main()


def advanced_training_features():
    """
    ğŸš€ Advanced Training Features for Production Language Models
    
    This section demonstrates additional techniques used in real-world language model training
    that go beyond the basic demo. These features are crucial for training large, capable models.
    """
    print("\n" + "=" * 80)
    print("ğŸš€ ADVANCED TRAINING FEATURES")
    print("=" * 80)
    print("Real production language models use many additional techniques:")
    
    print("\nğŸ¯ 1. GRADIENT ACCUMULATION")
    print("   â€¢ Simulate larger batch sizes when GPU memory is limited")
    print("   â€¢ Accumulate gradients over multiple forward passes before updating")
    print("   â€¢ Essential for training large models on consumer hardware")
    
    print("\nğŸ“Š 2. LEARNING RATE SCHEDULING")
    print("   â€¢ Cosine annealing: gradually reduce learning rate")
    print("   â€¢ Warmup: start with small learning rate, increase to target")
    print("   â€¢ Critical for stable training of large models")
    
    print("\nğŸ’¾ 3. GRADIENT CHECKPOINTING")
    print("   â€¢ Trade computation for memory by recomputing activations")
    print("   â€¢ Allows training much larger models on limited hardware")
    print("   â€¢ Essential for billion+ parameter models")
    
    print("\nğŸ² 4. ADVANCED TOKENIZATION")
    print("   â€¢ Byte-Pair Encoding (BPE): handles unknown words efficiently")
    print("   â€¢ SentencePiece: language-agnostic tokenization")
    print("   â€¢ Proper handling of special tokens, padding, truncation")
    
    print("\nğŸ“ˆ 5. VALIDATION AND EARLY STOPPING")
    print("   â€¢ Monitor performance on held-out validation data")
    print("   â€¢ Stop training when validation loss stops improving")
    print("   â€¢ Prevents overfitting to training data")
    
    print("\nğŸ”„ 6. MIXED PRECISION TRAINING")
    print("   â€¢ Use 16-bit floats for forward pass, 32-bit for gradients")
    print("   â€¢ Reduces memory usage and speeds up training")
    print("   â€¢ Maintains numerical stability")


def explain_attention_mechanism():
    """
    ğŸ§  Deep Dive: How Self-Attention Actually Works
    
    Provides an intuitive explanation of the attention mechanism with concrete examples.
    """
    print("\n" + "=" * 80)
    print("ğŸ§  DEEP DIVE: SELF-ATTENTION MECHANISM")
    print("=" * 80)
    
    print("Let's trace through exactly how attention works with a concrete example:")
    print("Sentence: 'The cat sat on the mat'")
    
    print("\nğŸ” STEP 1: CREATE QUERY, KEY, VALUE REPRESENTATIONS")
    print("Each word creates three vectors:")
    print("   â€¢ Query (Q): 'What am I looking for?'")
    print("   â€¢ Key (K): 'What do I represent?'") 
    print("   â€¢ Value (V): 'What information do I contain?'")
    
    print("\nğŸ¯ STEP 2: CALCULATE ATTENTION SCORES")
    print("For the word 'cat':")
    print("   â€¢ Its Query asks: 'What action am I doing?'")
    print("   â€¢ It compares with all Keys in the sentence")
    print("   â€¢ 'sat' Key responds: 'I represent an action!'")
    print("   â€¢ High similarity score between 'cat' Query and 'sat' Key")
    
    print("\nğŸ“Š STEP 3: APPLY SOFTMAX TO GET ATTENTION WEIGHTS")
    print("Raw scores: catâ†’cat(0.1), catâ†’sat(0.9), catâ†’on(0.2), ...")
    print("After softmax: catâ†’cat(0.05), catâ†’sat(0.7), catâ†’on(0.1), ...")
    print("(Numbers sum to 1.0, representing probability distribution)")
    
    print("\nğŸ­ STEP 4: WEIGHTED COMBINATION OF VALUES")
    print("'cat' gets new representation:")
    print("   new_cat = 0.05Ã—cat_value + 0.7Ã—sat_value + 0.1Ã—on_value + ...")
    print("   Result: 'cat' now understands it's the entity performing 'sitting'")
    
    print("\nğŸ”„ STEP 5: REPEAT FOR ALL WORDS IN PARALLEL")
    print("Every word simultaneously attends to every other word")
    print("Creates rich contextual representations for the entire sentence")
    
    print("\nğŸ¯ WHY MULTIPLE HEADS?")
    print("Different attention heads can focus on different relationships:")
    print("   â€¢ Head 1: Subject-Verb relationships (cat â†’ sat)")
    print("   â€¢ Head 2: Object-Preposition relationships (mat â†’ on)")
    print("   â€¢ Head 3: Modifier-Noun relationships (the â†’ cat)")
    print("   â€¢ Head 4: Long-distance dependencies")


def explain_training_dynamics():
    """
    ğŸ“ˆ Understanding Training Dynamics and Loss Curves
    
    Explains what happens during training and how to interpret the results.
    """
    print("\n" + "=" * 80)
    print("ğŸ“ˆ UNDERSTANDING TRAINING DYNAMICS")
    print("=" * 80)
    
    print("ğŸ¯ WHAT THE MODEL LEARNS DURING TRAINING:")
    print("   â€¢ Early epochs: Basic token patterns and frequencies")
    print("   â€¢ Middle epochs: Grammar rules and sentence structure")
    print("   â€¢ Later epochs: Semantic relationships and world knowledge")
    print("   â€¢ Advanced training: Reasoning and complex dependencies")
    
    print("\nğŸ“‰ INTERPRETING LOSS CURVES:")
    print("   â€¢ High initial loss (~log(vocab_size)): Random predictions")
    print("   â€¢ Rapid decrease: Learning basic patterns")
    print("   â€¢ Gradual decrease: Refining complex relationships")
    print("   â€¢ Plateau: Model has learned available patterns")
    
    print("\nğŸš¨ COMMON TRAINING ISSUES:")
    print("   â€¢ Loss not decreasing: Learning rate too high/low")
    print("   â€¢ Loss exploding: Gradient explosion, need clipping")
    print("   â€¢ Overfitting: Training loss << Validation loss")
    print("   â€¢ Underfitting: Both losses plateau at high values")
    
    print("\nğŸ”§ DEBUGGING STRATEGIES:")
    print("   â€¢ Start with tiny model and synthetic data")
    print("   â€¢ Monitor gradient norms and activation statistics")
    print("   â€¢ Use learning rate finders and schedulers")
    print("   â€¢ Implement proper validation and early stopping")


def scaling_to_production():
    """
    ğŸ­ Scaling to Production-Level Language Models
    
    Discusses the engineering challenges of training large language models.
    """
    print("\n" + "=" * 80)
    print("ğŸ­ SCALING TO PRODUCTION LANGUAGE MODELS")
    print("=" * 80)
    
    print("ğŸ—ï¸ MODEL SCALING LAWS:")
    print("   â€¢ GPT-3: 175 billion parameters, 300 billion tokens")
    print("   â€¢ Training cost: Millions of dollars in compute")
    print("   â€¢ Memory requirements: Hundreds of GPUs/TPUs")
    print("   â€¢ Training time: Weeks to months")
    
    print("\nâš¡ DISTRIBUTED TRAINING TECHNIQUES:")
    print("   â€¢ Data Parallelism: Split batches across GPUs")
    print("   â€¢ Model Parallelism: Split model layers across GPUs")
    print("   â€¢ Pipeline Parallelism: Pipeline model stages")
    print("   â€¢ Tensor Parallelism: Split individual operations")
    
    print("\nğŸ’¾ MEMORY OPTIMIZATION:")
    print("   â€¢ Gradient checkpointing: Recompute vs store activations")
    print("   â€¢ Parameter offloading: Move parameters to CPU/disk")
    print("   â€¢ Mixed precision: Use FP16 where possible")
    print("   â€¢ ZeRO optimizer: Distribute optimizer states")
    
    print("\nğŸ“Š DATA ENGINEERING:")
    print("   â€¢ Dataset size: Trillions of tokens from web crawls")
    print("   â€¢ Data cleaning: Remove duplicates, filter quality")
    print("   â€¢ Tokenization: Efficient encoding schemes (BPE)")
    print("   â€¢ Data loading: Efficient streaming from distributed storage")
    
    print("\nğŸ”„ INFRASTRUCTURE REQUIREMENTS:")
    print("   â€¢ Fault tolerance: Handle hardware failures gracefully")
    print("   â€¢ Monitoring: Track metrics across distributed systems")
    print("   â€¢ Checkpointing: Save model state for recovery")
    print("   â€¢ Networking: High-bandwidth interconnects")


if __name__ == "__main__":
    # Run the main training demo first
    main()
    
    # Then show additional educational content
    explain_attention_mechanism()
    explain_training_dynamics() 
    advanced_training_features()
    scaling_to_production()
    
    print("\n" + "=" * 80)
    print("ğŸ“ CONGRATULATIONS!")
    print("=" * 80)
    print("You've now seen a complete transformer implementation from scratch!")
    print("This covers the core concepts used in GPT, BERT, T5, and other models.")
    print("\nğŸš€ Next steps for your learning journey:")
    print("   1. Implement attention visualization tools")
    print("   2. Try training on real text datasets")
    print("   3. Experiment with different architectures")
    print("   4. Study recent papers on transformer improvements")
    print("   5. Build applications using pre-trained models")
    print("\nğŸ’¡ Remember: Every major language model uses these same core concepts!")
    print("The main differences are scale, training data, and fine-tuning strategies.")